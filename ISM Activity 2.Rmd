---
title: "Intermediate Statistical Methods"
author: "Students"
date: "9/30/2021"
output: html_document
params:
  x: 20
subtitle: Activity 2
bibliography: Biblio.bib
csl: apa.csl.txt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Introduction**

This activity explores the relationship between variables in a data set referred to Uber and Lyft rideshares in Boston, MA. The data set is a random sample of 2.500 observations from 693.071 cases in the original database, available at  <https://www.kaggle.com/brllrb/uber-and-lyft-dataset-boston-ma>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
urlfile<-'https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/rideshare_small.csv'
ride<-read.csv(url(urlfile))
library("easypackages")
paq <- c('e1071','lubridate', 'mosaic', 'ggformula', 'paramtest', 'pwr', 'ggplot2', 'nlme', 'dplyr', 'knitr', 'dplyr', 'dbplyr', 'readxl', 'GGally', 'Hmisc', 'corrplot', 'PerformanceAnalytics', 'statthink', 'tidyverse')
libraries(paq)
theme_set(theme_statthinking())
```

**Guiding question**

Does the distance of the ride explain variation in the price of the ride?

1. This assignment uses the same data from the first activity. Take a few minutes to reacquaint yourself with this analysis, including the linear regression estimates obtained.

```{r}
ride_lm <- lm(price ~ distance, data = ride)
coef(ride_lm)
summary(ride_lm)$r.square
summary(ride_lm)$sigma
```

The intercept is $10.44 and the slope is $2.89. The estimated price values will be:

$$
\hat{price} = 10.439 + 2.889 distance
$$
Moreover, the variability of prices accounted for distance is 12.44%, and the dispersion of the observed values of price respect with the predicted values by the model is, on average, $8.77.

Graphically, the relationship between price and distance look like:

```{r}
gf_point(price ~ distance, data = ride, size = 4, alpha = .2) %>%
  gf_labs(x = "Distance of the ride",
          y = "Price of the ride") %>%
  gf_smooth() %>%
  gf_smooth(method = 'lm', color = 'lightblue', linetype = 2)
```

2. Was the intercept from the linear regression fitted in activity 1 using the raw data as is interpretable in the context of the data? Rephrasing slightly, could the intercept term be made more interpretable? If so, how?

Table 1

```{r, message=FALSE}
st1 <- df_stats(~ price, data = ride, mean, min, max, median, sd, quantile(probs = c(0.25, 0.75)))
st2 <- df_stats(~ distance, data = ride, min, max, mean, median, sd, quantile(probs = c(0.25, 0.75)))
st <- rbind(st1, st2)
head(st)
```

Some descriptive statistics for the two variables included into the simple linear regression are shown in the Table 1. The minimum value of distance is close to 0 (0.03), which does the y-intercept quite interpretable. However, we can perform a minimum-centering transformation of distance to achieve a more interpretable y-intercept.
  
3. Fit a modified linear regression that performs a centering. That is, center the distance in some fashion. Options could include, minimum, mean, median, maximum, etc centering. What changed for the estimates obtained from the model?

```{r}
ride <- ride %>%
  mutate(distance_min = distance - min(distance))
ride_lm_min <- lm(price ~ distance_min, data = ride)
coef(ride_lm_min)
```

According to the context of the problem, on average, the price of the rides is $10.53 when distance travel is 0.03 miles (minimum value).

The new relationship look like:

```{r}
gf_point(price ~ distance_min, data = ride, size = 4, alpha = .15) %>%
  gf_labs(x = "Distance of the ride",
          y = "Price of the ride") %>%
  gf_smooth() %>%
  gf_smooth(method = 'lm', color = 'lightblue', linetype = 2)
```

As the structural relationship between the two variables has not changed, the slope still being the same across the models.

4. Which model, the centered or the uncentered, do you feel has a stronger justification for its usage? Be as specific as possible in your rationale. Use whichever model you feel is the best from #3, then extract from software or compute the standard errors for the estimated regression coefficients. Interpret these standard errors in the context of the problem.

The minimum-centered transformation of the distance is the most intuitive option. It slightly affects the y-intercept and offers more advantages than disadvantages: there are no negative values in the distribution and any descriptive statistic (mean or median) can be used to analyze the relationship between distance and price.

```{r}
sum_x <- ride %>%
    summarise(mean_distance_min = mean(distance_min),
              sum_dev_x_sq = sum((distance_min - mean_distance_min)^2))
sum_x
se_b0 <- sqrt(summary(ride_lm_min)$sigma^2 * ((1 / nrow(ride)) + ( sum_x[['mean_distance_min']]^2 / sum_x[['sum_dev_x_sq']]) ))
se_b1 <- sqrt(summary(ride_lm_min)$sigma^2 / sum_x[['sum_dev_x_sq']])

se_b0
se_b1
```

The standard errors (s.e.) of the estimated parameters are .37 for b0 and .15 for b1. This represents the uncertainty in the estimation of the population parameters B0 and B1. If another sample were extracted from the population, the expected variability between the current estimated parameters and the new ones will be, on average, .37 and .15 for b0 and b1 respectively.

5. Compute confidence intervals for the two regression coefficients. Justify your choice for level of confidence and interpret the confidence interval.

```{r}
coef(ride_lm_min)
summary(ride_lm_min)$coefficients[,2] 
CI_b0 <- 10.525840 + c(-1, 1) * 1.96 * 0.3784202
CI_b1 <- 2.889949 + c(-1, 1) * 1.96 * 0.1533785
CI_b0
CI_b1
```

The most common Confidence Interval in social sciences is at 95%, which means that if repeated samples were taken from the population, 95% of the new intervals will contain the true unknown value of the parameter B0 and B1 [@james2021statistical].

The interpretation of both CI are as follow:

- In the absence of any x value, the price of the rides would be, on average, somewhere between $16.67 and 17.36 (b0).
- With a 95% of confidence, a 1-mile increase in distance is associated with a price increase from $2.58 to 3.19 (b1).

6. Finally, perform hypothesis testing for the two regression coefficients. In this, set up the null/alternative hypotheses and then interpret the statistical results to provide a statistical conclusion. More specifically, what does the hypothesis test suggest about the null and alternative hypotheses?

The null hypothesis for the parameters are as follow:

$$
H_{0}: \beta_{0} = 0.\ The\  population\  yintercept\  equals\  0.
$$
or

$$ 
H_{0}: \beta_{1} = 0.\ The\  population\  slope\  equals\  0.
$$
In both cases, the alternative hypothesis is that the parameters are different from zero, in other words, there is a relationship between distance and price of rides.

```{r}
t_b0 <- 10.525840/ 0.3784202
t_b1 <- 2.889949/ 0.1533785
t_b0
t_b1
```

Considering the t-statistic results, in both cases the null hypothesis can be rejected. In particular, the t-statistic of B0 (96.87) falls far away of the interval constructed with 95% of confidence; on the other hand, the t-statistic of B1 (18.84) also falls out of the interval constructed at 95% of confidence.

In addition, p-values support this interpretation (<.001). In other words, probably the B0 and B1 parameters are not equal zero in the population. Finally, based on the CI, t-statistics and p-values, it can be concluded that there is some relationship between price and distance of rides. 

```{r}
summary(ride_lm_min)
```

The first part talked about the statistical conclusions, now compare/contrast this with practical conclusions. More specifically, would these results be useful in practice? Be as specific as possible in your response.

The results of the minimum-centered model are useful to explain part of the variability of prices using the information of miles traveled (distance) In practice, would be useful for the users or clients of the apps.

**References**