---
title: "ISM Activity 3"
author: "ss"
date: "10/13/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
ride <- readr::read_csv("https://raw.githubusercontent.com/lebebr01/psqf_6243/main/data/rideshare_small.csv")
library("easypackages")
paq <- c('e1071','lubridate', 'mosaic', 'ggformula', 'paramtest', 'pwr', 'ggplot2', 'nlme', 'knitr', 'dplyr', 'dbplyr', 'readxl', 'GGally', 'Hmisc', 'corrplot', 'PerformanceAnalytics', 'statthink', 'tidyverse')
libraries(paq)
theme_set(theme_statthinking())
```

**Introduction**

This activity explores the relationship between variables in a data set referred to Uber and Lyft rideshares in Boston, MA. The data set is a random sample of 2.500 observations from 693.071 cases in the original database, available at  <https://www.kaggle.com/brllrb/uber-and-lyft-dataset-boston-ma>

**Guiding Question**

Does the distance of the ride explain variation in the price of the ride?

Questions

1. This assignment uses the same data from the first and second activities. Take a few minutes to reacquaint yourself with these analyses, including the linear regression estimates obtained.

Based on the guiding question and previous analysis, the linear regressions results are as follow:

```{r}
ride_lm <- lm(price ~ distance, data = ride)
coef(ride_lm)
summary(ride_lm)$r.square
summary(ride_lm)$sigma
```

The intercept is 10.44 and the slope is 2.89. The estimated price values will be:

$$
\hat{price} = 10.439 + 2.889 \ distance
$$

Moreover, the variability of prices accounted for distance is $R^2$ = 12.44%, and the dispersion of the observed values of price respect with the predicted values by the model is, on average, $8.77.

Graphically, the relationship between price and distance look like:

```{r}
gf_point(price ~ distance, data = ride, size = 4, alpha = .2) %>%
  gf_labs(x = "Distance of the ride",
          y = "Price of the ride") %>%
  gf_smooth(method = 'loess') %>%
  gf_smooth(method = 'lm', color = 'lightblue', linetype = 2)
```

2. Extract the residuals from one of the linear regressions using the centered or uncentered regressions (it doesn’t matter which one). Perform a descriptive analysis on these residuals. This could include graphically visualizing the distribution and/or computing summary statistics on the residuals.

```{r}
ride$residuals <- resid(ride_lm)
```

Visualization of residuals (uncentered raw residuals):

Figure 1: Density plot

```{r}
gf_density(~ residuals, data = ride) %>%
  gf_labs(x = "Residuals")
```

Figure 2: QQ plot

```{r}
ggplot(ride, aes(sample = residuals)) + 
  stat_qq(size = 5) + 
  stat_qq_line(size = 2)
```

3.Evaluate the residuals for the assumption of normality of the residuals. How may the results be impacted by any deviation of normality?

Based on the figures, we cannot assume that the normality assumptions is meet,however, due to the sample size (n=2,500) and CLT theorem we can rely on the estimates from the model. For this reasons, the results will not be impacted by violating this assumption.

4. Evaluate the residuals for the assumption of homogeneity of variance. How may the results be impacted by any deviation of homogeneity of variance?

Figure 3. Scale-Location

```{r}
gf_point(.std.resid ~ .fitted, data = norm_residuals, size = 4, alpha = .15) %>%
  gf_hline(yintercept = ~ 2, color = 'red', size = 1) %>%
  gf_hline(yintercept = ~ -2, color = 'red', size = 1) %>%
  gf_smooth(method = 'loess', size = 2) %>%
  gf_labs(x = 'Fitted Values',
          y = 'Standardized Residuals')
```

Based on figure 3, most of the standardized residuals (approximately 95%) fall within -/+ 2 standard deviations. Moreover, the residuals are nearly equally distributed around the loess line. If we could not meet this assumption, thus the estimation of the S.E. will be biased and the inferences will not be accurate.

5. Are there any noticeable trends in the residuals? What implications could this have for the analysis? If a trend in the residuals exists, what may be a cause for this trend?

Based on Figure 3, there is no noticeable trend in the standardized residuals. If a trend appears, so we would be violating the assumption that error term is uncorrelated with predictor values. On top of that, it would be a signal that we are missing some predictor variable in the model.

6. Explore the model to identify if there are any points that have high leverage or could be identified as extreme values. It may be worth exploring studentized residuals, cook’s distance, and/or leverage values. 

Figure 4: Cook's distance

```{r}
norm_residuals %>%
  mutate(obs_num = 1:n()) %>%
  gf_col(.cooksd ~ obs_num, fill = 'black', color = 'black') %>%
  gf_labs(x = "Observation Number",
          y = "Cook's Distance")
```

```{r, echo=FALSE, results='hide'}
ride$student_residuals <- rstudent(ride_lm)
head(ride)
```

Figure 5. Studentized residuals

```{r}
ride %>%
  mutate(obs_num = 1:n()) %>%
  gf_point(student_residuals ~ obs_num, size = 5, alpha = .15) %>%
  gf_hline(yintercept = ~ 3, color = 'blue', size = 2) %>%
  gf_labs(x = "Observation Number",
          y = "Studentized Residuals")
```

Figure 6. Leverage

```{r}
norm_residuals %>%
  mutate(obs_num = 1:n()) %>%
  gf_col(.hat ~ obs_num, fill = 'black', color = 'black') %>%
  gf_labs(x = "Observation Number",
          y = "Hat Values (leverage)")
```

Based on Figures 4, 5 and 6, there are some potential outliers in the predictor variable. Some values of Distance (predictor) should be explored and evaluated in order to decide whether or not they would be removed from the data set to improve the estimates.